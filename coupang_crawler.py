import json
import time
import os
import subprocess
import random
import re
from playwright.sync_api import sync_playwright

# [NEW] S2B ë°ì´í„° ë³´ê°• ëª¨ë“ˆ ì„í¬íŠ¸
from data_enricher import S2B_Enricher 

# ======================================================
# [ì„¤ì •] í¬ë¡¤ë§ íƒ€ê²Ÿ ë° ìš´ì˜ ì •ì±…
# ======================================================
TARGET_URLS = [
    "https://www.coupang.com/vp/products/8610798143?itemId=19665760789&vendorItemId=86771432026&q=%EC%A0%84%EC%9E%90%EB%A0%88%EC%9D%B8%EC%A7%80&searchId=d027098a15810727&sourceType=search&itemsCount=36&searchRank=2&rank=2&traceId=mlg787wn",
    "https://www.coupang.com/vp/products/7249246657?itemId=18436391484&vendorItemId=92006548412&q=%EC%84%A0%ED%92%8D%EA%B8%B0&searchId=c4876bb75295792&sourceType=search&itemsCount=36&searchRank=2&rank=2&traceId=mlg78m1r",
    "coupang.com/vp/products/8036829511?itemId=23843669090&vendorItemId=90869617914&q=ì‚¼ì„±%20ë…¸íŠ¸ë¶&searchId=a93c62df4465418&sourceType=search&itemsCount=36&searchRank=1&rank=1&traceId=mlj3nf0o"
    # ... í•„ìš”í•œ URL ê³„ì† ì¶”ê°€
]

OUTPUT_FILE = 's2b_results.json'
CDP_PORT = 9222
CDP_URL = f"http://127.0.0.1:{CDP_PORT}"

# [í™˜ê²½] í¬ë¡¬ ê²½ë¡œ
CHROME_PATH = r"C:\Program Files\Google\Chrome\Application\chrome.exe"
CHROME_USER_DIR = r"C:\ChromeDev"

# [ì •ì±…] ì•ˆì •ì„± ì„¤ì •
RESTART_EVERY_N = 50      
BATCH_SLEEP_EVERY_N = 10 
BATCH_SLEEP_DURATION = 60 

# ======================================================
# [ëª¨ë“ˆ 1] ë¸Œë¼ìš°ì € ìƒëª…ì£¼ê¸° ê´€ë¦¬
# ======================================================
def launch_chrome():
    print(f"ğŸš€ [System] Chrome ì‹¤í–‰ ì¤‘... (Port: {CDP_PORT})")
    if not os.path.exists(CHROME_PATH):
        print(f"âŒ í¬ë¡¬ ì‹¤í–‰ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {CHROME_PATH}")
        return False

    cmd = [
        CHROME_PATH,
        f"--remote-debugging-port={CDP_PORT}",
        f"--user-data-dir={CHROME_USER_DIR}",
        "--no-first-run",
        "--no-default-browser-check",
        "--window-size=1920,1080"
    ]
    try:
        subprocess.Popen(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        time.sleep(3)
        return True
    except Exception as e:
        print(f"    âŒ Chrome ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return False

def kill_chrome():
    print("â™»ï¸ [System] ë©”ëª¨ë¦¬ ì´ˆê¸°í™”ë¥¼ ìœ„í•´ Chrome ì¬ì‹œì‘ ì¤€ë¹„...")
    try:
        subprocess.run(
            'wmic process where "name=\'chrome.exe\' and commandline like \'%ChromeDev%\'" call terminate',
            shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL
        )
        time.sleep(2)
    except: pass

# ======================================================
# [ëª¨ë“ˆ 2] ë°ì´í„° ì •ë°€ ì¶”ì¶œê¸° (Regex & All-Table Scan)
# ======================================================
def extract_all_specs(page):
    info_dict = {}
    try:
        rows = page.locator("table tr").all()
        for row in rows:
            try:
                texts = row.locator("th, td").all_inner_texts()
                if len(texts) >= 2:
                    key = texts[0].strip()
                    val = texts[1].strip()
                    if key and val:
                        info_dict[key] = val
            except: continue
    except: pass

    try:
        items = page.locator("ul.prod-description-attribute > li").all_inner_texts()
        for item in items:
            if ":" in item:
                parts = item.split(":", 1)
                info_dict[parts[0].strip()] = parts[1].strip()
    except: pass
    return info_dict

def extract_kc_by_regex(text):
    patterns = [
        r"[A-Z]{2}[0-9]{4,5}-[0-9]{4,5}[A-Z]?",
        r"[A-Z]{2,4}-[A-Z]{3}-[A-Z]{3}-[\w]+",
        r"R-R-[\w]+-[\w]+"
    ]
    found = set()
    for pat in patterns:
        matches = re.findall(pat, text)
        for m in matches:
            found.add(m)
    return " / ".join(list(found))

def get_best_value(info_dict, keywords, default_val=""):
    for key, val in info_dict.items():
        if any(kw in key for kw in keywords):
            if val and "ìƒì„¸" not in val and "ì°¸ì¡°" not in val:
                return val
    return default_val

# [NEW] ìƒì„¸ ì´ë¯¸ì§€ ì¶”ì¶œ í•¨ìˆ˜ (ë²„íŠ¼ í´ë¦­ + ìŠ¤í¬ë¡¤)
def get_detail_images_with_scroll(page):
    print("    ğŸ“œ [System] ìƒì„¸ ì´ë¯¸ì§€ í™•ë³´ ì‹œì‘...")
    
    # 1. 'ìƒí’ˆì •ë³´ ë”ë³´ê¸°' ë²„íŠ¼ í´ë¦­
    try:
        more_btns = page.locator("button, a").filter(has_text=re.compile(r"ìƒí’ˆì •ë³´|ë”ë³´ê¸°|í¼ì¹˜ê¸°")).all()
        clicked = False
        for btn in more_btns:
            if btn.is_visible():
                btn.click(force=True)
                clicked = True
                break
        if clicked:
            print("    ğŸ–±ï¸ 'ìƒí’ˆì •ë³´ ë”ë³´ê¸°' ë²„íŠ¼ í´ë¦­ ì„±ê³µ")
            time.sleep(2)
    except: pass

    # 2. ìŠ¤í¬ë¡¤ ë‹¤ìš´ (Lazy Loading ìœ ë„)
    try:
        page.evaluate("""async () => {
            await new Promise((resolve) => {
                let totalHeight = 0;
                const distance = 300;
                const timer = setInterval(() => {
                    const scrollHeight = document.body.scrollHeight;
                    window.scrollBy(0, distance);
                    totalHeight += distance;
                    if(totalHeight >= scrollHeight || totalHeight > 30000){
                        clearInterval(timer);
                        resolve();
                    }
                }, 100);
            });
        }""")
        time.sleep(2)
    except: pass

    # 3. ì´ë¯¸ì§€ URL ì¶”ì¶œ
    detail_images = []
    try:
        # ì£¼ìš” ì»¨í…Œì´ë„ˆ íƒìƒ‰
        containers = page.locator("#productDetail, .product-detail-content-border, #vendorInventory").all()
        if not containers:
            # ì»¨í…Œì´ë„ˆë¥¼ ëª» ì°¾ìœ¼ë©´ ë°”ë”” ì „ì²´ì—ì„œ ê²€ìƒ‰ (ì°¨ì„ ì±…)
            containers = [page.locator("body")]

        for cont in containers:
            imgs = cont.locator("img").all()
            for img in imgs:
                src = img.get_attribute("src") or img.get_attribute("data-src")
                if src and "http" in src and ".gif" not in src and "blank" not in src:
                    if src not in detail_images:
                        detail_images.append(src)
    except Exception as e:
        print(f"    âš ï¸ ì´ë¯¸ì§€ ì¶”ì¶œ ì¤‘ ì—ëŸ¬: {e}")
        
    return detail_images

# ======================================================
# [í•µì‹¬] í¬ë¡¤ë§ ë¡œì§ (Phase 1 ì „ìš©)
# ======================================================
def crawl_item(page, url): 
    print(f"â–¶ ì´ë™: {url[:60]}...")
    try:
        page.goto(url, wait_until="domcontentloaded", timeout=10000)
    except: pass 

    item = {
        "url": url, "name": "N/A", "price": 0, "image": "", 
        "kc": "ìƒì„¸ì„¤ëª…ì°¸ì¡°", "maker": "í˜‘ë ¥ì—…ì²´", "origin": "ì¤‘êµ­", "model": "",
        "g2b_code": "", "category": "ê¸°íƒ€",
        "detail_images": [] 
    }

    try:
        if "/login/" in page.url:
            print("    âš ï¸ ë¡œê·¸ì¸ í•„ìš” í˜ì´ì§€ -> ê±´ë„ˆëœ€")
            return None

        # JSON-LD íŒŒì‹±
        try:
            json_data = page.locator('script[type="application/ld+json"]').first.inner_text()
            data = json.loads(json_data)
            if isinstance(data, list): data = data[0]
            item["name"] = data.get("name", "N/A")
            item["image"] = data.get("image", "")
            if isinstance(item["image"], list): item["image"] = item["image"][0]
            offers = data.get("offers", {})
            if isinstance(offers, list): offers = offers[0]
            item["price"] = int(offers.get("price", 0))
        except: pass

        content = page.content()
        if "ë¬´ë£Œë°°ì†¡" not in content: item["price"] += 3000

        # [NEW] ìƒì„¸ ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤í–‰
        item["detail_images"] = get_detail_images_with_scroll(page)
        print(f"    ğŸ“¸ ìƒì„¸ ì´ë¯¸ì§€ {len(item['detail_images'])}ì¥ í™•ë³´")

        # ì •ë°€ ìŠ¤í™ ì¶”ì¶œ
        full_text = page.locator("body").inner_text()
        all_specs = extract_all_specs(page)
        
        model = get_best_value(all_specs, ["ëª¨ë¸ëª…", "ëª¨ë¸ë²ˆí˜¸", "í’ˆëª…"], "")
        if not model:
            match = re.search(r"\(([A-Za-z0-9-]{5,})\)", item["name"])
            if match: model = match.group(1)
        item["model"] = model

        item["maker"] = get_best_value(all_specs, ["ì œì¡°ì", "ìˆ˜ì…ì", "íŒë§¤ì—…ì", "ì œì¡°ì‚¬"], "í˜‘ë ¥ì—…ì²´")
        item["origin"] = get_best_value(all_specs, ["ì œì¡°êµ­", "ì›ì‚°ì§€", "êµ­ê°€"], "ì¤‘êµ­")

        kc_regex = extract_kc_by_regex(full_text)
        if kc_regex: item["kc"] = kc_regex

    except Exception as e:
        print(f"   âš ï¸ íŒŒì‹± ì—ëŸ¬: {e}")
        return None

    print(f"   âœ… ì¿ íŒ¡ ìˆ˜ì§‘ ì™„ë£Œ: {item['name'][:10]}... | ëª¨ë¸:{item['model']}")
    return item

# ======================================================
# [ì‹¤í–‰] ë©”ì¸ ë£¨í”„ (Phase 1 & Phase 2)
# ======================================================
def run_crawler():
    # --------------------------------------------------
    # [PHASE 1] ì¿ íŒ¡ ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘ (Playwright Context 1)
    # --------------------------------------------------
    print("\nğŸš€ [PHASE 1] ì¿ íŒ¡ ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘ ì‹œì‘...")
    
    urls_to_crawl = TARGET_URLS
    results = []

    # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    if os.path.exists(OUTPUT_FILE):
        try:
            with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
                saved_data = json.load(f)
                crawled_urls = set(item['url'] for item in saved_data)
                urls_to_crawl = [u for u in TARGET_URLS if u not in crawled_urls]
                results = saved_data
        except: pass

    if urls_to_crawl:
        kill_chrome()
        launch_chrome()
        
        with sync_playwright() as p:
            try:
                browser = p.chromium.connect_over_cdp(CDP_URL)
                context = browser.contexts[0]
                page = context.new_page()
                
                for i, url in enumerate(urls_to_crawl):
                    print(f"\n[{i+1}/{len(urls_to_crawl)}] ì²˜ë¦¬ ì¤‘...")
                    data = crawl_item(page, url) 
                    
                    if data:
                        results.append(data)
                        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
                            json.dump(results, f, ensure_ascii=False, indent=4)
                    
                    time.sleep(random.uniform(2, 4))
            except Exception as e:
                print(f"âŒ Phase 1 ì—ëŸ¬: {e}")
            finally:
                try: context.close()
                except: pass
                try: browser.close()
                except: pass
        
        kill_chrome() # ë¸Œë¼ìš°ì € ì™„ì „ ì¢…ë£Œ (ë¦¬ì†ŒìŠ¤ í•´ì œ)
        print("âœ… [PHASE 1] ì¿ íŒ¡ ìˆ˜ì§‘ ì™„ë£Œ. ë¸Œë¼ìš°ì € ì¢…ë£Œë¨.\n")
    else:
        print("ğŸ‰ ì‹ ê·œ ìˆ˜ì§‘í•  URLì´ ì—†ìŠµë‹ˆë‹¤. Phase 2ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.\n")

    # --------------------------------------------------
    # [PHASE 2] S2B ë°ì´í„° ë³´ê°• (Playwright Context 2)
    # --------------------------------------------------
    print("ğŸš€ [PHASE 2] S2B ë°ì´í„° ë³´ê°•(Enrichment) ì‹œì‘...")
    
    # S2B Enricher ì´ˆê¸°í™” (ìƒˆë¡œìš´ ë¸Œë¼ìš°ì € ì„¸ì…˜ ì‹œì‘)
    enricher = S2B_Enricher() 
    
    # ìµœì‹  ë°ì´í„° ë‹¤ì‹œ ë¡œë“œ
    if os.path.exists(OUTPUT_FILE):
        with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
            current_data = json.load(f)
    else:
        print("âŒ ì²˜ë¦¬í•  ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    updated_count = 0
    for idx, item in enumerate(current_data):
        # ëª¨ë¸ëª…ì´ ìˆê³  ì•„ì§ G2B ì½”ë“œê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ S2B ê²€ìƒ‰ ì‹œë„
        if item.get("model") and len(item["model"]) > 3 and not item.get("g2b_code"):
            
            print(f"ğŸ”¹ [{idx+1}/{len(current_data)}] S2B ê²€ìƒ‰: {item['model']}")
            s2b_data = enricher.fetch_s2b_details(item["model"])
            
            if s2b_data:
                print("    ğŸ‰ ë§¤ì¹­ ì„±ê³µ! ë°ì´í„° ë³‘í•© ì¤‘...")
                # S2B ë°ì´í„° ìš°ì„  ì ìš© (Golden Key)
                if s2b_data["category"]: item["category"] = s2b_data["category"]
                if s2b_data["manufacturer"]: item["maker"] = s2b_data["manufacturer"]
                if s2b_data["origin"]: item["origin"] = s2b_data["origin"]
                if s2b_data["g2b_code"]: item["g2b_code"] = s2b_data["g2b_code"]
                
                # KC ì •ë³´ ë³‘í•©
                s2b_kc_strs = [f"{k['category']}:{k['code']}" for k in s2b_data["kc_list"]]
                if s2b_kc_strs:
                    current_kc = item["kc"].split(" / ") if item["kc"] != "ìƒì„¸ì„¤ëª…ì°¸ì¡°" else []
                    combined = list(set(current_kc + s2b_kc_strs))
                    item["kc"] = " / ".join(combined)
                
                updated_count += 1
            else:
                print("    âš ï¸ ë§¤ì¹­ ì‹¤íŒ¨. ê¸°ì¡´ ë°ì´í„° ìœ ì§€.")
            
            # ì¤‘ê°„ ì €ì¥ (ë°ì´í„° ë³´í˜¸)
            with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
                json.dump(current_data, f, ensure_ascii=False, indent=4)
            
            time.sleep(1) # S2B ì„œë²„ ë¶€í•˜ ë°©ì§€
        else:
            print(f"    Pass: ëª¨ë¸ëª… ì—†ìŒ or ì´ë¯¸ ì™„ë£Œë¨ ({item.get('name')[:10]}...)")

    print(f"\nğŸ‰ ì „ì²´ ì‘ì—… ì¢…ë£Œ! ì´ {len(current_data)}ê°œ ì¤‘ {updated_count}ê°œ ë³´ê°•ë¨.")

if __name__ == "__main__":
    run_crawler()