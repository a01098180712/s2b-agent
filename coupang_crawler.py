import json
import time
import os
import subprocess
import random
import re # [ì¶”ê°€] ì •ê·œí‘œí˜„ì‹ ì‚¬ìš©
from playwright.sync_api import sync_playwright

# ======================================================
# [ì„¤ì •] í¬ë¡¤ë§ íƒ€ê²Ÿ ë° ìš´ì˜ ì •ì±…
# ======================================================
TARGET_URLS = [
    "https://www.coupang.com/vp/products/8610798143?itemId=19665760789&vendorItemId=86771432026&q=%EC%A0%84%EC%9E%90%EB%A0%88%EC%9D%B8%EC%A7%80&searchId=d027098a15810727&sourceType=search&itemsCount=36&searchRank=2&rank=2&traceId=mlg787wn",
    "https://www.coupang.com/vp/products/7249246657?itemId=18436391484&vendorItemId=92006548412&q=%EC%84%A0%ED%92%8D%EA%B8%B0&searchId=c4876bb75295792&sourceType=search&itemsCount=36&searchRank=2&rank=2&traceId=mlg78m1r",
    "https://www.coupang.com/vp/products/6359373947?itemId=13418949659&vendorItemId=92995378125&q=%EB%85%B8%ED%8A%B8%EB%B6%81&searchId=e154f8483813228&sourceType=search&itemsCount=36&searchRank=2&rank=2&traceId=mlg7936e",
    # ... ì¶”ê°€ URL
]

OUTPUT_FILE = 's2b_results.json'
CDP_PORT = 9222
CDP_URL = f"http://127.0.0.1:{CDP_PORT}"

# [í™˜ê²½] í¬ë¡¬ ê²½ë¡œ
CHROME_PATH = r"C:\Program Files\Google\Chrome\Application\chrome.exe"
CHROME_USER_DIR = r"C:\ChromeDev"

# [ì •ì±…] ì•ˆì •ì„± ì„¤ì •
RESTART_EVERY_N = 50      
BATCH_SLEEP_EVERY_N = 10 
BATCH_SLEEP_DURATION = 60 

# ======================================================
# [ëª¨ë“ˆ 1] ë¸Œë¼ìš°ì € ìƒëª…ì£¼ê¸° ê´€ë¦¬
# ======================================================
def launch_chrome():
    print(f"ğŸš€ [System] Chrome ì‹¤í–‰ ì¤‘... (Port: {CDP_PORT})")
    if not os.path.exists(CHROME_PATH):
        print(f"âŒ í¬ë¡¬ ì‹¤í–‰ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {CHROME_PATH}")
        return False

    cmd = [
        CHROME_PATH,
        f"--remote-debugging-port={CDP_PORT}",
        f"--user-data-dir={CHROME_USER_DIR}",
        "--no-first-run",
        "--no-default-browser-check",
        "--window-size=1920,1080"
    ]
    try:
        subprocess.Popen(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        time.sleep(3)
        return True
    except Exception as e:
        print(f"    âŒ Chrome ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return False

def kill_chrome():
    print("â™»ï¸ [System] ë©”ëª¨ë¦¬ ì´ˆê¸°í™”ë¥¼ ìœ„í•´ Chrome ì¬ì‹œì‘ ì¤€ë¹„...")
    try:
        subprocess.run(
            'wmic process where "name=\'chrome.exe\' and commandline like \'%ChromeDev%\'" call terminate',
            shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL
        )
        time.sleep(2)
    except: pass

# ======================================================
# [ëª¨ë“ˆ 2] ë°ì´í„° ì •ë°€ ì¶”ì¶œê¸° (Regex & All-Table Scan)
# ======================================================
def extract_all_specs(page):
    """
    í˜ì´ì§€ ë‚´ì˜ ëª¨ë“  í…Œì´ë¸”ê³¼ ìŠ¤í™ ë¦¬ìŠ¤íŠ¸ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ í†µí•© ì¶”ì¶œ
    """
    info_dict = {}
    
    # 1. ëª¨ë“  í…Œì´ë¸” ìŠ¤ìº” (í‘œ í˜•íƒœ ì •ë³´)
    try:
        rows = page.locator("table tr").all()
        for row in rows:
            try:
                # th-td êµ¬ì¡° ë˜ëŠ” td-td êµ¬ì¡° ëª¨ë‘ ëŒ€ì‘
                texts = row.locator("th, td").all_inner_texts()
                if len(texts) >= 2:
                    key = texts[0].strip()
                    val = texts[1].strip()
                    if key and val:
                        info_dict[key] = val
            except: continue
    except: pass

    # 2. ìƒë‹¨ ìŠ¤í™ ë¦¬ìŠ¤íŠ¸ (ul > li í˜•íƒœ)
    try:
        items = page.locator("ul.prod-description-attribute > li").all_inner_texts()
        for item in items:
            if ":" in item:
                parts = item.split(":", 1)
                info_dict[parts[0].strip()] = parts[1].strip()
    except: pass
    
    return info_dict

def extract_kc_by_regex(text):
    """
    í˜ì´ì§€ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ KC ì¸ì¦ ë²ˆí˜¸ íŒ¨í„´ì„ ì°¾ì•„ëƒ„
    íŒ¨í„´ ì˜ˆ: HU07445-11007Z, MSIP-REI-SEC-ECOSOLO, R-R-Kp1-...
    """
    patterns = [
        r"[A-Z]{2}[0-9]{4,5}-[0-9]{4,5}[A-Z]?",  # ì•ˆì „ì¸ì¦ (ì˜ˆ: HU07445-11007Z)
        r"[A-Z]{2,4}-[A-Z]{3}-[A-Z]{3}-[\w]+",   # ì „ìíŒŒ ì í•©ì„± (ì˜ˆ: MSIP-REI-...)
        r"R-R-[\w]+-[\w]+"                       # ë°©ì†¡í†µì‹  (ì˜ˆ: R-R-SEC-...)
    ]
    
    found = set()
    for pat in patterns:
        matches = re.findall(pat, text)
        for m in matches:
            found.add(m)
            
    return " / ".join(list(found))

def get_best_value(info_dict, keywords, default_val=""):
    """ë”•ì…”ë„ˆë¦¬ì—ì„œ í‚¤ì›Œë“œ ë§¤ì¹­ (ìƒì„¸ì„¤ëª…ì°¸ì¡° ì œì™¸)"""
    for key, val in info_dict.items():
        if any(kw in key for kw in keywords):
            # 'ìƒì„¸ì„¤ëª…'ì´ë‚˜ 'ì°¸ì¡°'ê°€ ë“¤ì–´ê°„ ë¬´ì˜ë¯¸í•œ ê°’ì€ ë¬´ì‹œ
            if val and "ìƒì„¸" not in val and "ì°¸ì¡°" not in val:
                return val
    return default_val

def crawl_item(page, url):
    print(f"â–¶ ì´ë™: {url[:60]}...")
    try:
        page.goto(url, wait_until="domcontentloaded", timeout=5000)
    except: pass 

    # [1] ê¸°ë³¸ ì •ë³´ (JSON-LD ìš°ì„ )
    item = {
        "url": url, "name": "N/A", "price": 0, "image": "", 
        "kc": "ìƒì„¸ì„¤ëª…ì°¸ì¡°", "maker": "í˜‘ë ¥ì—…ì²´", "origin": "ì¤‘êµ­", "model": "ì—†ìŒ"
    }

    try:
        # ì„±ì¸ì¸ì¦ í˜ì´ì§€ ì²´í¬
        if "/login/" in page.url:
            print("    âš ï¸ ë¡œê·¸ì¸ í•„ìš” í˜ì´ì§€ -> ê±´ë„ˆëœ€")
            return None

        json_data = page.locator('script[type="application/ld+json"]').first.inner_text()
        data = json.loads(json_data)
        if isinstance(data, list): data = data[0]

        item["name"] = data.get("name", "N/A")
        item["image"] = data.get("image", "")
        if isinstance(item["image"], list): item["image"] = item["image"][0]

        offers = data.get("offers", {})
        if isinstance(offers, list): offers = offers[0]
        item["price"] = int(offers.get("price", 0))

        content = page.content()
        if "ë¬´ë£Œë°°ì†¡" not in content:
            item["price"] += 3000
            print("   - ë°°ì†¡ë¹„ 3,000ì› ì¶”ê°€ë¨")

    except Exception as e:
        print(f"   âš ï¸ ê¸°ë³¸ íŒŒì‹± ì‹¤íŒ¨: {e}")
        return None

    # [2] ì •ë°€ ìŠ¤í™ ì¶”ì¶œ (New Logic)
    try:
        # í˜ì´ì§€ ì „ì²´ í…ìŠ¤íŠ¸ í™•ë³´ (Regexìš©)
        full_text = page.locator("body").inner_text()
        
        # ëª¨ë“  í…Œì´ë¸”/ìŠ¤í™ ì •ë³´ ë”•ì…”ë„ˆë¦¬í™”
        all_specs = extract_all_specs(page)
        
        # 1. KC ì¸ì¦ (Regex + Table ì¡°í•©)
        kc_from_table = get_best_value(all_specs, ["ì¸ì¦", "í—ˆê°€", "ì‹ ê³ ", "KC"], "")
        kc_from_regex = extract_kc_by_regex(full_text) # ì •ê·œì‹ìœ¼ë¡œ í˜ì´ì§€ ì „ì²´ ìŠ¤ìº”
        
        # ì •ê·œì‹ ê²°ê³¼ë¥¼ ìš°ì„ í•˜ë˜, í…Œì´ë¸” ì •ë³´ë„ ë³‘í•©
        kc_combined = set()
        if kc_from_regex: kc_combined.update(kc_from_regex.split(" / "))
        if kc_from_table: kc_combined.add(kc_from_table)
        
        if kc_combined:
            # 'ìƒì„¸ì„¤ëª…ì°¸ì¡°' ê°™ì€ ì“°ë ˆê¸° ë°ì´í„° ì œê±°
            clean_kc = [k for k in kc_combined if "ìƒì„¸" not in k and "ì°¸ì¡°" not in k]
            if clean_kc: item["kc"] = " / ".join(clean_kc)

        # 2. ì œì¡°ì‚¬ (ìš°ì„ ìˆœìœ„: ì‚¼ì„±/LG ë“± ë¸Œëœë“œ > í˜‘ë ¥ì—…ì²´)
        maker = get_best_value(all_specs, ["ì œì¡°ì", "ìˆ˜ì…ì", "íŒë§¤ì—…ì", "ì œì¡°ì‚¬"], "")
        # ì œì¡°ì‚¬ì— 'ì‚¼ì„±', 'LG' ë“±ì´ í¬í•¨ë˜ë©´ ê·¸ ê°’ì„ ì‚´ë¦¼. ì—†ìœ¼ë©´ í˜‘ë ¥ì—…ì²´.
        if maker: item["maker"] = maker
        else:
            # í…ìŠ¤íŠ¸ì—ì„œ 'ì‚¼ì„±ì „ì' ê°™ì€ ë¸Œëœë“œê°€ ë³´ì´ë©´ ì¶”ì¶œ ì‹œë„ (ê°„ë‹¨ ì˜ˆì‹œ)
            if "ì‚¼ì„±ì „ì" in full_text: item["maker"] = "ì‚¼ì„±ì „ì"
            elif "LGì „ì" in full_text: item["maker"] = "LGì „ì"

        # 3. ì›ì‚°ì§€
        origin = get_best_value(all_specs, ["ì œì¡°êµ­", "ì›ì‚°ì§€", "êµ­ê°€"], "")
        if origin: item["origin"] = origin

        # 4. ëª¨ë¸ëª… (í…Œì´ë¸” > ì œëª© > Regex)
        model = get_best_value(all_specs, ["ëª¨ë¸ëª…", "ëª¨ë¸ë²ˆí˜¸", "í’ˆëª…"], "")
        if not model:
            # ì œëª©ì— ëª¨ë¸ëª…ì´ ìˆëŠ” ê²½ìš°ê°€ ë§ìŒ (ì˜ˆ: ... ë‹¤ì´ì–¼ì‹ 23L (MS23C...))
            # ê´„í˜¸ ì•ˆì˜ ì˜ë¬¸+ìˆ«ì íŒ¨í„´ ì‹œë„
            match = re.search(r"\(([A-Za-z0-9-]{5,})\)", item["name"])
            if match: model = match.group(1)
        
        if model: item["model"] = model

    except Exception as e:
        print(f"   âš ï¸ ìƒì„¸ì •ë³´ ì •ë°€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")

    print(f"   âœ… ìˆ˜ì§‘ ì™„ë£Œ: {item['name'][:10]}... (ëª¨ë¸:{item['model']} / KC:{item['kc'][:15]}...)")
    return item

# ======================================================
# [ì‹¤í–‰] ë©”ì¸ ë£¨í”„
# ======================================================
def run_crawler():
    urls_to_crawl = TARGET_URLS
    results = []

    if os.path.exists(OUTPUT_FILE):
        try:
            with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
                saved_data = json.load(f)
                crawled_urls = set(item['url'] for item in saved_data)
                urls_to_crawl = [u for u in TARGET_URLS if u not in crawled_urls]
                results = saved_data
                if urls_to_crawl:
                    print(f"ğŸ“‚ ê¸°ì¡´ ë°ì´í„° {len(saved_data)}ê°œ í™•ì¸. ì‹ ê·œ {len(urls_to_crawl)}ê°œ ìˆ˜ì§‘ ì‹œì‘.")
        except: pass

    if not urls_to_crawl:
        print("ğŸ‰ ëª¨ë“  URLì´ ì´ë¯¸ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return

    total_count = len(urls_to_crawl)
    
    for i in range(0, total_count, RESTART_EVERY_N):
        chunk = urls_to_crawl[i : i + RESTART_EVERY_N]
        
        kill_chrome()
        launch_chrome()
        
        with sync_playwright() as p:
            try:
                browser = p.chromium.connect_over_cdp(CDP_URL)
                context = browser.contexts[0]
                page = context.new_page()
                
                for j, url in enumerate(chunk):
                    global_idx = i + j + 1
                    
                    if global_idx > 1 and (global_idx - 1) % BATCH_SLEEP_EVERY_N == 0:
                        print(f"\nâ˜• [Break] {BATCH_SLEEP_EVERY_N}ê°œ ìˆ˜ì§‘ ì™„ë£Œ. {BATCH_SLEEP_DURATION}ì´ˆ íœ´ì‹...")
                        time.sleep(BATCH_SLEEP_DURATION)
                    
                    data = crawl_item(page, url)
                    if data:
                        results.append(data)
                        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
                            json.dump(results, f, ensure_ascii=False, indent=4)
                    
                    time.sleep(random.uniform(2, 5))

            except Exception as e:
                print(f"âŒ ë¸Œë¼ìš°ì € ì—°ê²°/ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
                continue

    print(f"\nğŸ‰ ì „ì²´ ì‘ì—… ì™„ë£Œ! ì´ {len(results)}ê°œ ì €ì¥ë¨: {OUTPUT_FILE}")

if __name__ == "__main__":
    run_crawler()