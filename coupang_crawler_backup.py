import json
import time
import os
import subprocess
import random
import re
from playwright.sync_api import sync_playwright

# ======================================================
# [ì„¤ì •]
# ======================================================
TARGET_URLS = [
    "https://www.coupang.com/vp/products/7410479243?itemId=19199625171&vendorItemId=86317012667",
    "https://www.coupang.com/vp/products/9124094477?itemId=26840740061&vendorItemId=93127986643",
    "https://www.coupang.com/vp/products/8466469683?itemId=24496597951&vendorItemId=91538631793"
]

OUTPUT_FILE = 's2b_results.json'
CDP_PORT = 9222
CDP_URL = f"http://127.0.0.1:{CDP_PORT}"
CHROME_PATH = r"C:\Program Files\Google\Chrome\Application\chrome.exe"
CHROME_USER_DIR = r"C:\ChromeDev"

RESTART_EVERY_N = 50      
BATCH_SLEEP_EVERY_N = 10 
BATCH_SLEEP_DURATION = 60 

# ======================================================
# [ëª¨ë“ˆ] ë¸Œë¼ìš°ì € ì œì–´
# ======================================================
def launch_chrome():
    print(f"ğŸš€ [System] Chrome ì‹¤í–‰ ì¤‘... (Port: {CDP_PORT})")
    if not os.path.exists(CHROME_PATH):
        print(f"âŒ í¬ë¡¬ ì‹¤í–‰ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {CHROME_PATH}")
        return False

    cmd = [
        CHROME_PATH,
        f"--remote-debugging-port={CDP_PORT}",
        f"--user-data-dir={CHROME_USER_DIR}",
        "--no-first-run",
        "--no-default-browser-check",
        "--window-size=1920,1080"
    ]
    try:
        subprocess.Popen(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        time.sleep(3)
        return True
    except: return False

def kill_chrome():
    try:
        subprocess.run('wmic process where "name=\'chrome.exe\' and commandline like \'%ChromeDev%\'" call terminate', shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        time.sleep(2)
    except: pass

# ======================================================
# [ëª¨ë“ˆ 2] ë°ì´í„° ì¶”ì¶œ
# ======================================================
def extract_all_specs(page):
    info_dict = {}
    try:
        rows = page.locator("table tr").all()
        for row in rows:
            try:
                texts = row.locator("th, td").all_inner_texts()
                if len(texts) >= 2:
                    info_dict[texts[0].strip()] = texts[1].strip()
            except: continue
    except: pass

    try:
        items = page.locator("ul.prod-description-attribute > li").all_inner_texts()
        for item in items:
            if ":" in item:
                parts = item.split(":", 1)
                info_dict[parts[0].strip()] = parts[1].strip()
    except: pass
    return info_dict

def extract_kc_by_regex(text):
    patterns = [
        r"[A-Z]{2}[0-9]{4,5}-[0-9]{4,5}[A-Z]?", 
        r"[A-Z]{2,4}-[A-Z]{3}-[A-Z]{3}-[\w]+",
        r"R-R-[\w]+-[\w]+"
    ]
    found = set()
    for pat in patterns:
        found.update(re.findall(pat, text))
    return " / ".join(list(found))

def get_best_value(info_dict, keywords, default_val=""):
    for key, val in info_dict.items():
        if any(kw in key for kw in keywords):
            if val and "ìƒì„¸" not in val and "ì°¸ì¡°" not in val:
                return val
    return default_val

def crawl_item(page, url):
    print(f"â–¶ ì´ë™: {url[:60]}...")
    try:
        page.goto(url, wait_until="domcontentloaded", timeout=5000)
    except: pass 

    item = {
        "url": url, "name": "N/A", "price": 0, "image": "", 
        "kc": "ìƒì„¸ì„¤ëª…ì°¸ì¡°", "maker": "í˜‘ë ¥ì—…ì²´", "origin": "ì¤‘êµ­", "model": "ì—†ìŒ",
        "category": "ë¯¸ë¶„ë¥˜" # [ì¶”ê°€] ì¹´í…Œê³ ë¦¬ í•„ë“œ
    }

    try:
        if "/login/" in page.url:
            print("    âš ï¸ ë¡œê·¸ì¸ í•„ìš” í˜ì´ì§€ -> ê±´ë„ˆëœ€")
            return None

        # [ì¶”ê°€] ì¿ íŒ¡ ì¹´í…Œê³ ë¦¬(Breadcrumb) ì¶”ì¶œ
        try:
            # ì¿ íŒ¡ì˜ breadcrumb idëŠ” ë³´í†µ 'breadcrumb'
            breadcrumb = page.locator("#breadcrumb").first.inner_text()
            # ì¤„ë°”ê¿ˆ ë“±ì„ '>' ë¡œ ë³€ê²½í•˜ì—¬ ê¹”ë”í•˜ê²Œ ì •ë¦¬
            item["category"] = breadcrumb.replace("\n", " > ").strip()
        except:
            item["category"] = "ë¯¸ë¶„ë¥˜"

        # JSON-LD íŒŒì‹±
        json_data = page.locator('script[type="application/ld+json"]').first.inner_text()
        data = json.loads(json_data)
        if isinstance(data, list): data = data[0]

        item["name"] = data.get("name", "N/A")
        item["image"] = data.get("image", "")
        if isinstance(item["image"], list): item["image"] = item["image"][0]

        offers = data.get("offers", {})
        if isinstance(offers, list): offers = offers[0]
        item["price"] = int(offers.get("price", 0))

        content = page.content()
        if "ë¬´ë£Œë°°ì†¡" not in content:
            item["price"] += 3000
            print("   - ë°°ì†¡ë¹„ 3,000ì› ì¶”ê°€ë¨")

    except Exception as e:
        print(f"   âš ï¸ ê¸°ë³¸ íŒŒì‹± ì‹¤íŒ¨: {e}")
        return None

    try:
        full_text = page.locator("body").inner_text()
        all_specs = extract_all_specs(page)
        
        # KC
        kc_table = get_best_value(all_specs, ["ì¸ì¦", "í—ˆê°€", "ì‹ ê³ ", "KC"], "")
        kc_regex = extract_kc_by_regex(full_text)
        kc_combined = set()
        if kc_regex: kc_combined.update(kc_regex.split(" / "))
        if kc_table: kc_combined.add(kc_table)
        clean_kc = [k for k in kc_combined if "ìƒì„¸" not in k and "ì°¸ì¡°" not in k]
        if clean_kc: item["kc"] = " / ".join(clean_kc)

        # Maker
        maker = get_best_value(all_specs, ["ì œì¡°ì", "ìˆ˜ì…ì", "íŒë§¤ì—…ì", "ì œì¡°ì‚¬"], "")
        if maker: item["maker"] = maker
        else:
            if "ì‚¼ì„±ì „ì" in full_text: item["maker"] = "ì‚¼ì„±ì „ì"
            elif "LGì „ì" in full_text: item["maker"] = "LGì „ì"

        # Origin
        origin = get_best_value(all_specs, ["ì œì¡°êµ­", "ì›ì‚°ì§€", "êµ­ê°€"], "")
        if origin: item["origin"] = origin

        # Model
        model = get_best_value(all_specs, ["ëª¨ë¸ëª…", "ëª¨ë¸ë²ˆí˜¸", "í’ˆëª…"], "")
        if not model:
            match = re.search(r"\(([A-Za-z0-9-]{5,})\)", item["name"])
            if match: model = match.group(1)
        if model: item["model"] = model

    except Exception as e:
        print(f"   âš ï¸ ìƒì„¸ì •ë³´ ë¶„ì„ ì˜¤ë¥˜: {e}")

    print(f"   âœ… ìˆ˜ì§‘: {item['name'][:10]}... (ì¹´í…Œê³ ë¦¬: {item['category'][:15]}...)")
    return item

def run_crawler():
    urls_to_crawl = TARGET_URLS
    results = []

    if os.path.exists(OUTPUT_FILE):
        try:
            with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
                saved = json.load(f)
                done = set(i['url'] for i in saved)
                urls_to_crawl = [u for u in TARGET_URLS if u not in done]
                results = saved
                if urls_to_crawl: print(f"ğŸ“‚ ê¸°ì¡´ {len(saved)}ê°œ ìœ ì§€. ì‹ ê·œ {len(urls_to_crawl)}ê°œ ì‹œì‘.")
        except: pass

    if not urls_to_crawl:
        print("ğŸ‰ ëª¨ë“  URL ì™„ë£Œ.")
        return

    chunk_size = RESTART_EVERY_N
    for i in range(0, len(urls_to_crawl), chunk_size):
        chunk = urls_to_crawl[i : i + chunk_size]
        kill_chrome()
        launch_chrome()
        
        with sync_playwright() as p:
            try:
                browser = p.chromium.connect_over_cdp(CDP_URL)
                context = browser.contexts[0]
                page = context.new_page()
                
                for j, url in enumerate(chunk):
                    if (i + j) > 0 and (i + j) % BATCH_SLEEP_EVERY_N == 0:
                        print(f"\nâ˜• íœ´ì‹ {BATCH_SLEEP_DURATION}ì´ˆ...")
                        time.sleep(BATCH_SLEEP_DURATION)
                    
                    data = crawl_item(page, url)
                    if data:
                        results.append(data)
                        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
                            json.dump(results, f, ensure_ascii=False, indent=4)
                    
                    time.sleep(random.uniform(2, 5))
            except: continue

    print(f"\nğŸ‰ ì™„ë£Œ. ì´ {len(results)}ê°œ ì €ì¥.")

if __name__ == "__main__":
    run_crawler()