import time
import os
import subprocess
import requests
import re  # ì •ê·œí‘œí˜„ì‹ ëª¨ë“ˆ ì¶”ê°€
from io import BytesIO
from PIL import Image
from playwright.sync_api import sync_playwright

# ======================================================
# [ì„¤ì •]
# ======================================================
TEST_URL = "https://www.coupang.com/vp/products/8610798143?itemId=19665760789&vendorItemId=86771432026"
CDP_PORT = 9222
CDP_URL = f"http://127.0.0.1:{CDP_PORT}"
OUTPUT_FILENAME = "merged_detail_v26.jpg"

CHROME_PATH = r"C:\Program Files\Google\Chrome\Application\chrome.exe"
CHROME_USER_DIR = r"C:\ChromeDev"

# ======================================================
# [ê¸°ëŠ¥ 1] í¬ë¡¬ ìë™ ì‹¤í–‰
# ======================================================
def ensure_chrome_running():
    print(f"â™»ï¸ [System] Chrome ìƒíƒœ ì ê²€...")
    try:
        requests.get(f"{CDP_URL}/json/version", timeout=1)
        print("    âœ… Chromeì´ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.")
        return
    except:
        print("    â„¹ï¸ Chrome ì‹¤í–‰ ì‹œì‘...")

    if not os.path.exists(CHROME_PATH):
        print(f"    âŒ ì˜¤ë¥˜: í¬ë¡¬ ê²½ë¡œ í™•ì¸ í•„ìš”: {CHROME_PATH}")
        return

    cmd = [
        CHROME_PATH,
        f"--remote-debugging-port={CDP_PORT}",
        f"--user-data-dir={CHROME_USER_DIR}",
        "--no-first-run",
        "--window-size=1920,1080"
    ]
    try:
        subprocess.Popen(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        time.sleep(3)
    except Exception as e:
        print(f"    âŒ ì‹¤í–‰ ì‹¤íŒ¨: {e}")

# ======================================================
# [ê¸°ëŠ¥ 2] ì´ë¯¸ì§€ ë³‘í•©
# ======================================================
def merge_images_vertical(image_urls):
    print(f"\nğŸ§© [Merger] ì¶”ì¶œëœ {len(image_urls)}ì¥ì˜ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ...")
    valid_images = []
    
    headers = {
        "User-Agent": "Mozilla/5.0",
        "Referer": "https://www.coupang.com/"
    }
    
    for i, url in enumerate(image_urls):
        try:
            if url.startswith("//"): url = "https:" + url
            
            # íƒ€ì„ì•„ì›ƒ 5ì´ˆ
            response = requests.get(url, headers=headers, timeout=5)
            
            if response.status_code == 200:
                img = Image.open(BytesIO(response.content)).convert("RGB")
                
                # [í•„í„°ë§]
                # HTML ì†ŒìŠ¤ì—ì„œ ê¸ì–´ì˜¤ë©´ ì•„ì´ì½˜ì´ë‚˜ ì‘ì€ ì¥ì‹ ìš”ì†Œë„ í¬í•¨ë  ìˆ˜ ìˆìœ¼ë‹ˆ
                # í¬ê¸° í•„í„°ë§ì„ ë°˜ë“œì‹œ í•´ì•¼ í•©ë‹ˆë‹¤.
                if img.width >= 300: 
                    valid_images.append(img)
                    print(f"   âœ… [í™•ë³´] {url[-30:]} ({img.width}x{img.height})")
                else:
                    # print(f"   âŒ [íƒˆë½] ë„ˆë¬´ ì‘ìŒ: {img.width}x{img.height}")
                    pass
            else:
                print(f"   âš ï¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {url}")
        except: pass

    if not valid_images:
        print("âŒ ë³‘í•©í•  ìœ íš¨í•œ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ìº”ë²„ìŠ¤ ìƒì„±
    max_width = max(img.width for img in valid_images)
    total_height = sum(img.height for img in valid_images)
    
    print(f"   ğŸ“ ìµœì¢… ìº”ë²„ìŠ¤: {max_width}x{total_height}px (ì´ {len(valid_images)}ì¥)")
    
    merged_img = Image.new('RGB', (max_width, total_height), (255, 255, 255))
    y_offset = 0
    for img in valid_images:
        if img.width != max_width:
            new_height = int(img.height * (max_width / img.width))
            img = img.resize((max_width, new_height), Image.LANCZOS)
        
        merged_img.paste(img, (0, y_offset))
        y_offset += img.height

    merged_img.save(OUTPUT_FILENAME, quality=90)
    print(f"\nâœ… [ì„±ê³µ] ì €ì¥ ì™„ë£Œ: {OUTPUT_FILENAME}")

# ======================================================
# [ë©”ì¸] V26 ë¡œì§ (HTML Raw String Parsing)
# ======================================================
def test_v26_html_parsing():
    print(f"ğŸ§ª [Test V26] HTML ì›ë¬¸ ì¶”ì¶œ ë° ì •ê·œì‹(Regex) íŒŒì‹±")
    ensure_chrome_running()
    print(f"ğŸ”— URL: {TEST_URL}")

    with sync_playwright() as p:
        try:
            print(f"ğŸ”Œ Chrome ì—°ê²° ì¤‘...")
            browser = p.chromium.connect_over_cdp(CDP_URL)
            context = browser.contexts[0]
            if context.pages: page = context.pages[0]
            else: page = context.new_page()

            if TEST_URL not in page.url:
                page.goto(TEST_URL, wait_until="domcontentloaded")
                time.sleep(2)

            # 1. ë²„íŠ¼ í´ë¦­ (ì¼ë‹¨ ë‚´ìš©ì€ ë¡œë”©ì‹œì¼œì•¼ í•¨)
            print("    ğŸ” ë²„íŠ¼ í´ë¦­ ì‹œë„...")
            try:
                btn = page.locator(".product-detail-etc-view-btn, #productDetail button").first
                if btn.is_visible():
                    btn.click(force=True)
                    print("    ğŸ–±ï¸ ë²„íŠ¼ í´ë¦­ ì™„ë£Œ (3ì´ˆ ëŒ€ê¸°)")
                    time.sleep(3)
            except: pass

            # 2. ìŠ¤í¬ë¡¤ (HTML ë¡œë”© ìœ ë„)
            print("    ğŸ“œ HTML ë°ì´í„° í™•ë³´ë¥¼ ìœ„í•œ ìŠ¤í¬ë¡¤...")
            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            time.sleep(2)
            page.evaluate("window.scrollTo(0, document.body.scrollHeight)") 
            time.sleep(2)

            # 3. [í•µì‹¬] ìƒì„¸ ì˜ì—­ì˜ HTML ì†ŒìŠ¤ì½”ë“œ ì „ì²´ë¥¼ ë¬¸ìì—´ë¡œ ê°€ì ¸ì˜´
            print("    ğŸ“¥ ìƒì„¸ ì˜ì—­ HTML ì†ŒìŠ¤ì½”ë“œ ì¶”ì¶œ ì¤‘...")
            
            # .product-detail-content-inside ì˜ì—­ì˜ HTMLì„ í†µì§¸ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.
            # innerHTMLì€ í˜„ì¬ ë¸Œë¼ìš°ì €ê°€ ì•Œê³  ìˆëŠ” ëª¨ë“  íƒœê·¸ êµ¬ì¡°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
            detail_html = page.evaluate("""() => {
                const container = document.querySelector('.product-detail-content-inside');
                return container ? container.innerHTML : "";
            }""")
            
            if not detail_html:
                # ë§Œì•½ ìœ„ í´ë˜ìŠ¤ê°€ ì—†ë‹¤ë©´ vendorInventory ì‹œë„
                print("    âš ï¸ 1ì°¨ ì˜ì—­ ì—†ìŒ, ë°±ì—… ì˜ì—­(#vendorInventory) ì‹œë„...")
                detail_html = page.evaluate("""() => {
                    const container = document.querySelector('#vendorInventory');
                    return container ? container.innerHTML : "";
                }""")

            print(f"    ğŸ“„ í™•ë³´ëœ HTML ê¸¸ì´: {len(detail_html)}ì")

            # 4. [Python] ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì´ë¯¸ì§€ URL ê°•ì œ ì¶”ì¶œ
            # íŒ¨í„´: httpë¡œ ì‹œì‘í•˜ê³ , ì¤‘ê°„ì— "ë‚˜ ' ê°™ì€ê²Œ ì—†ê³ , jpg/png/gif ë“±ìœ¼ë¡œ ëë‚˜ëŠ” ë¬¸ìì—´
            print("    ğŸ§¬ ì •ê·œì‹(Regex)ìœ¼ë¡œ ì´ë¯¸ì§€ URL ë°œêµ´ ì¤‘...")
            
            # íŒ¨í„´ ì„¤ëª…:
            # http[s]? : http ë˜ëŠ” https
            # :// : ://
            # [^"'\s<>]+ : ë”°ì˜´í‘œ, ê³µë°±, êº½ì‡ ê°€ ì•„ë‹Œ ë¬¸ìê°€ ì—°ì†ë¨
            # \. : ì (.)
            # (?:jpg|jpeg|png|gif|bmp|webp) : í™•ì¥ì (ê·¸ë£¹í™”í•˜ë˜ ìº¡ì²˜ ì•ˆ í•¨)
            pattern = r'(https?://[^"\'\s<>]+\.(?:jpg|jpeg|png|gif|bmp|webp))'
            
            found_urls = re.findall(pattern, detail_html)
            
            # ì¤‘ë³µ ì œê±° ë° í•„í„°ë§
            candidate_urls = []
            seen = set()
            
            for url in found_urls:
                # URL ì •ì œ (ê°€ë” ì¿¼ë¦¬ìŠ¤íŠ¸ë§ ë“±ì´ ë¶™ì–´ìˆì„ ìˆ˜ ìˆìŒ)
                clean_url = url.split('?')[0] # ë¬¼ìŒí‘œ ë’¤ ì œê±° (ì„ íƒì‚¬í•­, ì¼ë‹¨ ìœ ì§€)
                
                # ì¿ íŒ¡ CDN ë„ë©”ì¸ í™•ì¸ (ì™¸ë¶€ ê´‘ê³  ì œì™¸ ëª©ì )
                if 'coupangcdn.com' in url or 'vendor_inventory' in url or 'retail' in url:
                    if url not in seen:
                        candidate_urls.append(url)
                        seen.add(url)
            
            print(f"    ğŸ” HTML ë¶„ì„ ê²°ê³¼: {len(candidate_urls)}ê°œì˜ ì´ë¯¸ì§€ ì£¼ì†Œ ë°œê²¬!")
            
            # 5. ë³‘í•©
            if candidate_urls:
                merge_images_vertical(candidate_urls)
            else:
                print("âŒ HTML ì†ŒìŠ¤ ë‚´ì—ì„œ ì´ë¯¸ì§€ URLì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    test_v26_html_parsing()